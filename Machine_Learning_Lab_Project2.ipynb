{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Lab Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "sriuDeZKOFE1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JWuzyk/ML-Lab-Facial-Keypoint-Detection/blob/master/Machine_Learning_Lab_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVk5Q8W4Aq42",
        "colab_type": "text"
      },
      "source": [
        "Tasks:\n",
        "  - Preprocessing ()\n",
        "  - Data Augmentation (partially done) see https://imgaug.readthedocs.io/en/latest/source/examples_keypoints.html (Reflection, Rotation, Contrast Jittering?)\n",
        "  - Different Models for each keypoint\n",
        "  - Transfer Learning (partially done) - Could add more refinement in the imported network, see keras documentation, need to retrain some of layers in the network, consider using other networks, potentially one pretrained on faces not ImageNet ,eg. VGGFace https://github.com/rcmalli/keras-vggface\n",
        "  - Hyperparameter Tuning either sklearn https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ or with Tensorboard  see https://www.youtube.com/watch?v=BqgTU7_cBnk (learing rate, optimiser, augmentation ) + saving model weights\n",
        "  - Do a proper validartion split\n",
        "  - Set up Kaggle submission\n",
        "  - Use an emsemble model\n",
        "  - Find More Data e.g. http://www.milbo.org/muct/,https://github.com/soheillll/Facial-Keypoint-Detection,https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints, http://umdfaces.io/\n",
        "\n",
        "Stretch Goals\n",
        "  - Combine with face detection and apply to other images\n",
        "  \n",
        "Challenge:\n",
        "  - Lots of the data is missing values so we can't train on all of it and there is no sensible way to fill in missing data\n",
        "  \n",
        "Solution: \n",
        "  - Train individual models for each feature\n",
        "  \n",
        "Resources:\n",
        "  -http://cs231n.stanford.edu/reports/2016/pdfs/010_Report.pdf ; basically a paper by someone doing exactly what we are doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1zhtO7jROxz",
        "colab_type": "text"
      },
      "source": [
        "## Load data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtwklNQKxIPq",
        "colab_type": "code",
        "outputId": "ea589764-9bfe-460e-aa2f-ef4eef634ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# Load from github\n",
        "\n",
        "\n",
        "!git clone https://github.com/JWuzyk/ML-Lab-Facial-Keypoint-Detection/\n",
        "%cd ML-Lab-Facial-Keypoint-Detection/ \n",
        "\n",
        "!unzip data/test.zip \n",
        "!unzip data/training.zip\n",
        "\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML-Lab-Facial-Keypoint-Detection'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 24 (delta 5), reused 9 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n",
            "/content/ML-Lab-Facial-Keypoint-Detection\n",
            "Archive:  data/test.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  data/training.zip\n",
            "  inflating: training.csv            \n",
            "data  Machine_Learning_Lab_Project.ipynb  README.md  test.csv  training.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2NGg0r1zpQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading in the data from my drive as pandas Dataframes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IDLookupTable = pd.read_csv('data/IdLookupTable.csv')\n",
        "Training = pd.read_csv('training.csv')\n",
        "Test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TapQPb0L1BET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#separate data into different parts (so we can use different models given that some data is missing)\n",
        "\n",
        "#separatedata gets rid of the rows containing nan values for subsection of Data_Frame between col1 and col2\n",
        "def separatedata(df, col1, col2):\n",
        "  sel = df.dropna(subset=[*df.columns[col1:col2]])\n",
        "  sel = sel.reset_index(drop=True)\n",
        "  \n",
        "  keys = sel.iloc[:,col1:col2]\n",
        "  keys = keys.to_numpy()\n",
        "  \n",
        "  images = sel.iloc[:,-1]\n",
        "  images = images.apply(lambda n: np.fromstring(n, dtype = int, sep=' '))\n",
        "  images = np.vstack(images.values)\n",
        "  images = images.reshape(images.shape[0],96,96)\n",
        "  \n",
        "  # I run into issues running the model if I don't have an array with shape (-,96,96,3), \n",
        "  ret = np.empty((*images.shape, 3), dtype=np.uint8)\n",
        "  ret[:, :, :,:] = images[:,:, :, np.newaxis]\n",
        "  images = ret\n",
        "  \n",
        "  return images,keys\n",
        "\n",
        "def normalize(x, y):\n",
        "  x = x/255\n",
        "  y = y/96\n",
        "  \n",
        "  return (x, y)\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYnteO3FERM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I played around a bit and think we should separate the data as follows, can also think about splitting the eyebrow data if we want to\n",
        "\n",
        "x_eye_center, y_eye_center = separatedata(Training, 0, 4)\n",
        "#x_eye_corner, y_eye_corner = separatedata(Training, 4, 12)\n",
        "#x_eyebrow, y_eyebrow = separatedata(Training, 12, 20)\n",
        "#x_nose_tip, y_nose_tip = separatedata(Training, 20, 22)\n",
        "#x_mouth_corner_top, y_mouth_corner_top = separatedata(Training, 22, 28)\n",
        "#x_mouth_center_bottom_lip, y_mouth_center_bottom_lip = separatedata(Training, 28, 30)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O61K4RCEiUC-",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOyrVl-75P4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from imgaug.augmentables import Keypoint, KeypointsOnImage\n",
        "\n",
        "# Data Augmentation \n",
        "\n",
        "def generate(x,y):\n",
        "  images = (x).astype(np.uint8)\n",
        "  keys = y\n",
        "  #ia.imshow(image)\n",
        "\n",
        "  n_images = images.shape[0]\n",
        "  n_keys = keys.shape[1]//2\n",
        "\n",
        "  keys = keys.reshape(n_images,n_keys,2)\n",
        "\n",
        "  keypoints = []\n",
        "  for k in range(n_images):\n",
        "    kps = KeypointsOnImage([Keypoint(*keys[k,i,:]) for i in range(n_keys)], shape=images.shape)\n",
        "    keypoints.append(kps)\n",
        "\n",
        "  # rotate each image some angle between -30 and 30 degrees and flip horizontaly 50% of the time\n",
        "  seq = iaa.Sequential([\n",
        "      iaa.Affine(rotate=(-30,30)),\n",
        "      iaa.Fliplr(0.5)\n",
        "  ])\n",
        "\n",
        "  image_aug, kps_aug = seq(images=images, keypoints=keypoints)\n",
        "\n",
        "  # image with keypoints before/after augmentation (shown below)\n",
        "  #image_before = keypoints[30].draw_on_image(images[30], size=2)\n",
        "  #image_after = kps_aug[30].draw_on_image(image_aug[30], size=2)\n",
        "  #ia.imshow(image_before)\n",
        "  #ia.imshow(image_after)\n",
        "\n",
        "  keys_aug = np.array([keypoints[i].to_xy_array() for i in range(len(keypoints))]).reshape(len(keypoints),n_keys*2)\n",
        "\n",
        "\n",
        "  # add augmented data to training data\n",
        "  x_train = np.concatenate((x,image_aug))\n",
        "  y_train = np.concatenate((y,keys_aug))\n",
        "  \n",
        "  return image_aug,keys_aug\n",
        "\n",
        "def augment(x_train,y_train):\n",
        "  x,y=generate(x_train,y_train)\n",
        "  x_ret = np.concatenate((x_train,x))\n",
        "  y_ret = np.concatenate((y_train,y))\n",
        "  return x_ret,y_ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sriuDeZKOFE1",
        "colab_type": "text"
      },
      "source": [
        "# Plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am2x10S64h2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "#Turning the data from a dataframe into x_train,y_train, x_test numpy arrays so that we can use it\n",
        "# x_train has shape (2140,96,96,3) 2140 images each 96x96 with 3 colour channels, y_train has shape (2140,30), in the form (x_1,y_1,...x_15,y_15) for the 15 keypoints\n",
        "\n",
        "# Copied the getimage method from https://github.com/shichaoji/img_extract/blob/master/img_extract.py, definitely is a faster way for reading in data but it's nice for viewing the images\n",
        "\n",
        "\n",
        "# Takes a string representing a 96x96 image as in the last column of the training.csv and returns a PIL image\n",
        "def getimage(each):\n",
        "    img = Image.new( 'RGB', (96,96), \"black\") \n",
        "    pixels = img.load() # create the pixel map\n",
        "    \n",
        "    cot=[int(i) for i in each.split(' ')]\n",
        "    for i in range(img.size[0]):    # for every pixel:\n",
        "        for j in range(img.size[1]):\n",
        "            pixels[i,j] = (cot[i+j*96],cot[i+j*96],cot[i+j*96]) # set the colour accordingly\n",
        "            \n",
        "    return img\n",
        "  \n",
        "def str_to_np(images):\n",
        "  # converts a string of integers to a numpy array representing it as an image\n",
        "   \n",
        "  n_rows = images.shape[0]\n",
        "  \n",
        "  np_images=np.zeros((n_rows,96,96,3))\n",
        "\n",
        "  for i in range(n_rows):\n",
        "    im = np.array(getimage(images[i]))\n",
        "    np_images[i] = im\n",
        "    \n",
        "  return np_images\n",
        "\n",
        "\n",
        "#There's also a ready made method dataframe.to_numpy()...I think it does exactly what this function does?\n",
        "def keys_to_np(keys):\n",
        "  n_rows, n_keys = keys.shape\n",
        "  \n",
        "  np_keys = np.zeros((n_rows,n_keys))\n",
        "  for i in range(n_rows):\n",
        "    np_keys[i] = np.array(keys.iloc[i,:])\n",
        "  return np_keys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goAcU2bsyy0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from PIL import ImageDraw\n",
        "\n",
        "def plotWithKeypoints1(data):\n",
        "\n",
        "  key = np.array(data.iloc[:-1])\n",
        "  key = key.astype(int).reshape(15,2)\n",
        "  \n",
        "  im = getimage(data['Image'])\n",
        "  \n",
        "  draw = ImageDraw.Draw(im)\n",
        "  for x,y in zip(key[:,0],key[:,1]):\n",
        "    draw.ellipse((x-1, y-1, x+1, y+1),fill = 'blue')\n",
        "  return im\n",
        "\n",
        "def plotWithKeypoints2(im,key):\n",
        "  \n",
        "  key = key.astype(int).reshape(15,2)\n",
        "  img = getimage(im)\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  for x,y in zip(key[:,0],key[:,1]):\n",
        "    draw.ellipse((x-1, y-1, x+1, y+1),fill = 'blue')\n",
        "  \n",
        "  return img\n",
        "\n",
        "plotWithKeypoints1(Training.iloc[100,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L52A2jZOISl",
        "colab_type": "text"
      },
      "source": [
        "# Create and Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYOau0ZRtEwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Conv2D, MaxPooling2D, Activation, Dropout, Flatten\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.xception import Xception\n",
        "from keras.models import Model\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrXw7BR9_ouW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code stolen from sheet 4 (was provided to us, not written by us)\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Create a plot showing the training history of `model.fit`.\n",
        "    \n",
        "    Example:\n",
        "        history = model.fit(...)\n",
        "        plot_history(history)\n",
        "    \"\"\"\n",
        "\n",
        "    plt.style.use(\"seaborn-poster\")\n",
        "    x = range(history.params['epochs'])\n",
        "#     acc, val_acc = history.history['acc'], history.history.get('val_acc')\n",
        "    f, axarr = plt.subplots(1, sharex=True)\n",
        "#     axarr[0].set_title('accuracy')\n",
        "#     axarr[0].plot(x, acc, label='train')\n",
        "#     if val_acc:\n",
        "#         axarr[0].plot(x, val_acc, label='validation')\n",
        "#     axarr[0].legend()\n",
        "    \n",
        "    loss, val_loss = history.history['loss'], history.history.get('val_loss')\n",
        "    axarr.set_title('loss')\n",
        "    axarr.plot(x, loss, label='train')\n",
        "    if val_loss:\n",
        "        axarr.plot(x, val_loss, label='validation')\n",
        "    axarr.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrsjzZNXX0uu",
        "colab_type": "text"
      },
      "source": [
        "## Set data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyvV8TRCX0Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # set data\n",
        "# x_eye_center, y_eye_center = separatedata(Training, 0, 4)\n",
        "# x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "# x_train, y_train = augment(x_train, y_train)\n",
        "# x_train,_ = normalize(x_train, y_train)\n",
        "# x_valid,_ = normalize(x_valid, y_valid)\n",
        "\n",
        "def return_data(data_selection):\n",
        "  \n",
        "  if data_selection == 'eye_center':\n",
        "    x_eye_center, y_eye_center = separatedata(Training, 0, 4)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "    \n",
        "  elif data_selection == 'eye_corner':\n",
        "    x_eye_corner, y_eye_corner = separatedata(Training, 4, 12)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "\n",
        "  elif data_selection == 'eyebrow':\n",
        "    x_eyebrow, y_eyebrow = separatedata(Training, 12, 20)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "    \n",
        "  elif data_selection == 'nose_tip':\n",
        "    x_nose_tip, y_nose_tip = separatedata(Training, 20, 22)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "    \n",
        "  elif data_selection == 'mouth_corner_top':\n",
        "    x_mouth_corner_top, y_mouth_corner_top = separatedata(Training, 22, 28)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "    \n",
        "  elif data_selection == 'mouth_center_bottom_lip':\n",
        "    x_mouth_center_bottom_lip, y_mouth_center_bottom_lip = separatedata(Training, 28, 30)\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "    \n",
        "  else:\n",
        "    print(\"data_selection must be one of the following:eye_center, eye_corner, eyebrow, nose_tip, mouth_corner_top, mouth_center_bottom_lip\")\n",
        "    return 0\n",
        "          \n",
        "  x_train, y_train = augment(x_train, y_train)\n",
        "  x_train,_ = normalize(x_train, y_train)\n",
        "  x_valid,_ = normalize(x_valid, y_valid)\n",
        "          \n",
        "  return x_train, y_train, x_valid, y_valid\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqud6XLL_Psh",
        "colab_type": "text"
      },
      "source": [
        "## **Xception based model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFc9WfkXxa3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer learning based on https://keras.io/applications/\n",
        "\n",
        "\n",
        "def make_model_Xception(n_keys):\n",
        "  # Create a model based on transfer learning from one of the built in keras models, note this section uses the functional API not Sequential model\n",
        "  # Xception is a cool model see https://www.youtube.com/watch?v=KfV8CJh7hE0\n",
        "  # Should replace Xception with VGG Face or at the very least retrain some of it\n",
        "  \n",
        "  #importing the base model, should double check that the input is fine\n",
        "  base_model = Xception(weights='imagenet', include_top=False)\n",
        "  x = base_model.output\n",
        "  \n",
        "  # add a Global Average Pooling for some reason\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "  # add a dense layer\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  \n",
        "  # a dense layer to compute the predicted keypoints\n",
        "  coords = Dense(n_keys, activation='linear')(x)\n",
        "  model = Model(inputs=base_model.input, outputs=coords)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "# remember to activate the GPU before training (edit -> notebook settings), runs really quick for me ~2 per epoch, loss went down to 20ish but stayed there\n",
        "# Tried Transfer learning, error went way down to 3ish, I suspect this is overfitting though, can't quite quantify this yet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_GcSwsGT6tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xception_model =  make_model_Xception(4)\n",
        "Xception_model.compile(optimizer = 'adam',loss='mean_squared_error')\n",
        "historyX = Xception_model.fit(x_train,y_train, batch_size=32, epochs=10, validation_data=(x_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya0c9R36UJ91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# historyX\n",
        "\n",
        "# plt.style.use(\"seaborn-poster\")\n",
        "# x = range(historyX.params['epochs'])\n",
        "\n",
        "# f, axarr = plt.subplots(1, sharex=True)\n",
        "\n",
        "    \n",
        "# loss, val_loss = historyX.history['loss'], historyX.history.get('val_loss')\n",
        "# axarr.set_title('loss')\n",
        "# axarr.plot(x, loss, label='train')\n",
        "# if val_loss:\n",
        "#   axarr.plot(x, val_loss, label='validation')\n",
        "# axarr.legend()\n",
        "\n",
        "plot_history(historyX)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz-HMNO6_CCm",
        "colab_type": "text"
      },
      "source": [
        "## **Home-made Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GALzfKBy-3bP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normal learning\n",
        "def make_model_custom(n_keys):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(n_keys))\n",
        "  model.add(Activation('linear'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S_3qQAiy6-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =  make_model_custom(4)\n",
        "model.compile(optimizer = 'adam',loss='mean_squared_error',metrics =['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94v7JMCsCuO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train,y_train, batch_size=32, epochs=15, validation_split = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIb7wityDqzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y19HI1OAy-WN",
        "colab_type": "text"
      },
      "source": [
        "## **VGG Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGT3f9UlzOhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "  \n",
        "from keras.engine import  Model\n",
        "from keras.layers import Flatten, Dense, Input, MaxPooling2D, Dropout\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bd6ILdbJOC2y",
        "colab": {}
      },
      "source": [
        "#function to give a model based on vgg face\n",
        "def make_model_VGGFace(n_keys,freeze = True):\n",
        "  \n",
        "  #importing the base model, should double check that the input is fine\n",
        "  #base_model = VGGFace(model='resnet50')\n",
        "  #x = base_model.output\n",
        "  \n",
        "  vgg_model = VGGFace(include_top=False, input_shape=(96, 96,3), weights = 'vggface')\n",
        "  x = vgg_model.get_layer('pool5').output\n",
        "  x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(512, activation= 'relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  out = Dense(n_keys, activation='linear')(x)\n",
        "   \n",
        "  custom_vgg_model = Model(vgg_model.input, output = out)\n",
        "  if freeze:\n",
        "    for layer in custom_vgg_model.layers[:-4]:\n",
        "      layer.trainable = False\n",
        "  \n",
        "  \n",
        "  return custom_vgg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6H3-2j5wByE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vggmodel =  make_model_VGGFace(4,freeze = False)\n",
        "vggmodel.compile(optimizer = 'adam',loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjpT1NEOJPwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eieMVbj1aFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#See how good the model looks\n",
        "\n",
        "history = vggmodel.fit(x_train,y_train, batch_size=32, epochs=5, validation_split = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDbOa_ZlA3c7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I am confused by the loss thing\n",
        "#I found an article explaining discrepancies between loss and accuracy that's quite nice\n",
        "#http://www.jussihuotari.com/2018/01/17/why-loss-and-accuracy-metrics-conflict/\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBWZzgO-IGZE",
        "colab_type": "text"
      },
      "source": [
        "# Optimising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GE9b3lCRUut",
        "colab_type": "text"
      },
      "source": [
        "## Using hyperas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfrvXuZ5BNWG",
        "colab_type": "code",
        "outputId": "fe2ff1be-6f03-42f0-cd59-cb3f720c3f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Using hyperas and the tutorial from this guy\n",
        "#https://towardsdatascience.com/keras-hyperparameter-tuning-in-google-colab-using-hyperas-624fa4bbf673\n",
        "\n",
        "!pip install hyperas\n",
        "!pip install hyperopt\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.5.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.5.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.5.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (6.0.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.16.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.2.4)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.4.2)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (41.0.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "\u001b[31mERROR: ipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: hyperas, prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "Successfully installed hyperas-0.4.1 prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.16.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJq-BJf5axCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xlgYBPnPAwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test):\n",
        "    '''\n",
        "    Model providing function:\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_shape=(784,)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
        "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "\n",
        "    # If we choose 'four', add an additional fourth layer\n",
        "    if {{choice(['three', 'four'])}} == 'four':\n",
        "        model.add(Dense(100))\n",
        "        model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, Y_train,\n",
        "              batch_size={{choice([64, 128])}},\n",
        "              nb_epoch=1,\n",
        "              verbose=2,\n",
        "              validation_data=(X_test, Y_test))\n",
        "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test accuracy:', acc)\n",
        "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgNEGt5oWctz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMmxzaX_TBlx",
        "colab_type": "code",
        "outputId": "161f1cf6-6f35-47cc-968a-d9e6fb722853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "data = return_data('eye_center')\n",
        "\n",
        "best_run, best_model = optim.minimize(model=model,\n",
        "                                          data=data,\n",
        "                                          max_evals=10,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          notebook_name='Machine Learning Lab Project', # This is important!\n",
        "                                          trials=Trials())\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-eed9bafde569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                           \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Machine Learning Lab Project'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# This is important!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                           trials=Trials())\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mmodel_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hyperopt_model_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mtemp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./temp_model.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mwrite_temp_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mget_hyperopt_model_string\u001b[0;34m(model, data, functions, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnotebook_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mnotebook_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/{}.ipynb\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mnotebook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_CONVERT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mexporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ML-Lab-Facial-Keypoint-Detection/Machine Learning Lab Project.ipynb'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUQXsATMRMGm",
        "colab_type": "text"
      },
      "source": [
        "## Using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC8lbp5OFYCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Parameters:\n",
        "   - Model [custom, VGGFace, Xception]\n",
        "   - optimizer ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'] \n",
        "   - learning rate \n",
        "   - batch size\n",
        "   - data augmentation []\n",
        "   - custom layers\n",
        "  \n",
        "variable learning rate??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYof9W2ZgJEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# set data\n",
        "x_eye_center, y_eye_center = separatedata(Training, 0, 4)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_eye_center, y_eye_center, test_size=0.33, shuffle= True)\n",
        "#x_train, y_train = augment(x_train, y_train)\n",
        "x_train,_ = normalize(x_train, y_train)\n",
        "x_valid,_ = normalize(x_valid, y_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EyqfzGpHYVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "def create_model(model_type = 'VGGFace', optimizer = 'RMSprop', lr = 0.001, n_keys=4):\n",
        "  \n",
        "  #model type\n",
        "  if model_type == 'VGGFace':\n",
        "    model =  make_model_VGGFace(n_keys)\n",
        "    \n",
        "  #optimiser  \n",
        "  if optimizer == 'adam':     \n",
        "    opt = optimizers.Adam(lr=lr)\n",
        "  if optimizer == 'RMSprop':     \n",
        "    opt = optimizers.RMSprop(lr=lr)\n",
        "  \n",
        "  model.compile(optimizer = opt,loss='mean_squared_error')\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VttKf2GsX4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Apparently it is way better to use https://github.com/maxpumperla/hyperas, feel free to replace this wiht that\n",
        "\n",
        "\n",
        "# Careful running search over too many parameters at once causes a crash\n",
        "\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = KerasRegressor(build_fn=create_model, epochs=10, verbose=1)\n",
        "# define the grid search parameters\n",
        "batch_size = [8,32,64]\n",
        "param_grid = dict(batch_size=batch_size)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3peZqloAyImK",
        "colab_type": "text"
      },
      "source": [
        "Should write something so that grid searc automatically saves it's results but from now\n",
        "\n",
        " Best optimiser after 10 epochs: RMSprop \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v3oQOhLyH_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU11U7CbkL_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "model =  create_model(optimizer = 'RMSprop')\n",
        "model.fit(x_train, y_train, epochs = 10, batch_size=10, validation_data=(x_valid, y_valid))\n",
        "model.evaluate(x=x_valid, y=y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1RF4coHYYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "15.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHU2VGbvGv1y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iu4Uafl4r_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "keys = model.predict(x_train[100].reshape(1,96,96,3))\n",
        "plt.imshow(plotWithKeypoints2(Training['Image'][100],keys))\n",
        "plt.show()\n",
        "plt.imshow(plotWithKeypoints1(Training.iloc[100,:]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}